<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
body {
	font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	font-weight: 300;
	font-size: 18px;
	margin-left: auto;
	margin-right: auto;
	width: 1000px;
}
h1 {
	font-weight: 300;
}
.disclaimerbox {
	background-color: #eee;
	border: 1px solid #eeeeee;
	border-radius: 10px;
	-moz-border-radius: 10px;
	-webkit-border-radius: 10px;
	padding: 20px;
}
video.header-vid {
	height: 140px;
	border: 1px solid black;
	border-radius: 10px;
	-moz-border-radius: 10px;
	-webkit-border-radius: 10px;
}
img.header-img {
	height: 140px;
	border: 1px solid black;
	border-radius: 10px;
	-moz-border-radius: 10px;
	-webkit-border-radius: 10px;
}
img.rounded {
	border: 0px solid #eeeeee;
	border-radius: 10px;
	-moz-border-radius: 10px;
	-webkit-border-radius: 10px;
}
a:link, a:visited {
	color: #1367a7;
	text-decoration: none;
}
a:hover {
	color: #208799;
}
td.dl-link {
	height: 160px;
	text-align: center;
	font-size: 22px;
}
.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
	box-shadow: 0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */ 5px 5px 0 0px #fff, /* The second layer */ 5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */ 10px 10px 0 0px #fff, /* The third layer */ 10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */ 15px 15px 0 0px #fff, /* The fourth layer */ 15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */ 20px 20px 0 0px #fff, /* The fifth layer */ 20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */ 25px 25px 0 0px #fff, /* The fifth layer */ 25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
	margin-left: 10px;
	margin-right: 45px;
}
.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
	box-shadow: 0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */ 5px 5px 0 0px #fff, /* The second layer */ 5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */ 10px 10px 0 0px #fff, /* The third layer */ 10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
	margin-top: 5px;
	margin-left: 10px;
	margin-right: 30px;
	margin-bottom: 5px;
}
.vert-cent {
	position: relative;
	top: 50%;
	transform: translateY(-50%);
}
hr {
	border: 0;
	height: 1px;
	background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
}
</style>
<html>
<head>
<title>Stochastic Adversarial Video Prediction</title>
<meta property="og:image" content="https://alexlee-gk.github.io/video_prediction/index_files/table_fig1.jpg" />
<meta property="og:title" content="Stochastic Adversarial Video Prediction. arXiv 1804.01523." />
</head>

<body>
<br>
<center>
  <span style="font-size:42px">Stochastic Adversarial Video Prediction</span><br>
  <table align=center width=1000px>
    <tr>
      <td align=center width=200px>
        <span style="font-size:20px"><a href="https://people.eecs.berkeley.edu/~alexlee_gk/">Alex X. Lee</a></span>
      </td>
      <td align=center width=200px>
        <span style="font-size:20px"><a href="https://richzhang.github.io/">Richard Zhang</a></span>
      </td>
      <td align=center width=200px>
        <span style="font-size:20px"><a href="https://febert.github.io/">Frederik Ebert</a></span>
      </td>
      <td align=center width=200px>
        <span style="font-size:20px"><a href="https://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a></span>
      </td>
      <td align=center width=200px>
        <span style="font-size:20px"><a href="https://people.eecs.berkeley.edu/~cbfinn/">Chelsea Finn</a></span>
      </td>
      <td align=center width=200px>
        <span style="font-size:20px"><a href="https://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a></span>
      </td>
    </tr>
  </table>
  <table align=center width=1000px>
    <tr>
      <td align=center width=200px>
        <span style="font-size:18px">University of California, Berkeley</span>
      </td>
    </tr>
  </table>
  <table align=center width=1000px>
    <tr>
      <td align=center width=275px>
        <span style="font-size:22px"></span>
      </td>
      <td align=center width=225px>
        <span style="font-size:22px">Code<a href='https://github.com/alexlee-gk/video_prediction'> [GitHub]</a></span>
      </td>
      <td align=center width=225px>
        <span style="font-size:22px">arXiv<a href="http://arxiv.org/abs/1804.01523"> [preprint]</a></span>
      </td>
      <td align=center width=275px>
        <span style="font-size:22px"></span>
      </td>
    </tr>
  </table>
</center>
<table align=center width=1000px>
  <tr>
    <td align=center width=400px>
      <img class="rounded" src = "./index_files/table_fig1.jpg" width="450px"></img>
    </td>
    <td align=center width=400px>
      <img class="rounded" src = "./index_files/table_fig1_alt.jpg" width="450px"></img>
    </td>
  </tr>
</table>
<br>
<hr>
<center>
  <h1>Abstract</h1>
</center>
Being able to predict what may happen in the future requires an in-depth understanding of the physical and causal rules that govern the world. A model that is able to do so has a number of appealing applications, from robotic planning to representation learning. However, learning to predict raw future observations, such as frames in a video, is exceedingly challenging&mdash;the ambiguous nature of the problem can cause a naively designed model to average together possible futures into a single, blurry prediction. Recently, this has been addressed by two distinct approaches: (a) latent variational variable models that explicitly model underlying stochasticity and (b) adversarially-trained models that aim to produce naturalistic images. However, a standard latent variable model can struggle to produce realistic results, and a standard adversarially-trained model underutilizes latent variables and fails to produce diverse predictions. We show that these distinct methods are in fact complementary. Combining the two produces predictions that look more realistic to human raters and better cover the range of possible futures. Our method outperforms prior and concurrent work in these aspects.
<br>
<hr>
<center>
  <h1>Try the Stochastic Adversarial Video Prediction (SAVP) Model</h1>
</center>
<table align=center width=900px>
  <tr>
    <td>
      <a href='https://github.com/alexlee-gk/video_prediction'><img class="round" style="width:900px" src="./index_files/method.jpg"/></a>
	  </td>
  </tr>
</table>
<br>
<table align=center width=800px>
  <tr>
    <td align=center>
      <span style="font-size:28px"> <a href='https://github.com/alexlee-gk/video_prediction'>[GitHub]</a></span>
    </td>
  </tr>
</table>
<br>
<hr>
<center>
  <h1>Paper</h1>
</center>
<table align=center width=500 px>
  <tr>
    <td>
    	<a href="http://arxiv.org/abs/1804.01523"><img class="layered-paper-big" style="height:175px" src="./index_files/1804.01523_page1.jpg"/></a>
    </td>
    <td>
    	<span style="font-size:12pt">A. X. Lee, R. Zhang, F. Ebert, <br> P. Abbeel, C. Finn, S. Levine</span><br>
      <span style="font-size:12pt"><b>Stochastic Adversarial Video Prediction.</b></span><br>
      <span style="font-size:12pt">arXiv (<a href="http://arxiv.org/abs/1804.01523">preprint</a>).</span>
    </td>
  </tr>
</table>
<br>
<table align=center width=600px>
  <tr>
    <td align=center>
        <span style="font-size:28px"><a href="./index_files/bibtex_arxiv2018.txt">[Bibtex]</a></span>
    </td>
  </tr>
</table>
<br>
<hr>
<center>
  <h1>Example Results</h1>
</center>
We show qualitative results of the video predictions achieved by our SAVP method, our GAN and VAE variants, and other approaches. SV2P is prior work from Babaeizadeh et al. 2017, while SVG is concurrent work from Denton & Fergus 2018. For the stochastic models, we show the prediction with the "best" similarity compared to the ground truth video (out of 100 samples), unless otherwise labeled. Yellow indicates predicted frames, and white indicates the conditioned frames. We also show that our model can predict several hundred frames into the future despite only being trained to predict 10 future frames.
<br>
<br>
<table align=center width=750px>
  <tr>
    <td align=center>
      <span style="font-size:22px">BAIR action-free robot pushing dataset</span>
    </td>
  </tr>
  <tr>
    <td align=center>
      <img src="./index_files/images/bair_action_free_00232.gif" width="100%"><br><br>
      <img src="./index_files/images/bair_action_free_00025.gif" width="100%"><br><br>
      <img src="./index_files/images/bair_action_free_00000.gif" width="100%"><br><br>
      <img src="./index_files/images/bair_action_free_00032.gif" width="100%"><br>
    </td>
  </tr>
  <tr>
    <td>
      <span style="font-size:18px;float:right">
        <a href="https://people.eecs.berkeley.edu/~alexlee_gk/projects/savp/tables/bair_action_free_all/index.html">[All test set]</a>
        <a href="https://people.eecs.berkeley.edu/~alexlee_gk/projects/savp/tables/bair_action_free_random/index.html">[Random samples]</a>
        <a href="https://people.eecs.berkeley.edu/~alexlee_gk/projects/savp/tables/bair_action_free_random_avg/index.html">[Diversity visualization]</a>
      </span>
    </td>
  </tr>
</table>
<br>
<table align=center width=468.75px>
  <tr>
    <td align=center>
      <span style="font-size:22px">BAIR action-free robot pushing dataset <br> (500 time-steps)</span>
    </td>
  </tr>
  <tr>
    <td align=center>
      <img src="./index_files/images/bair_action_free_long_random_00026.gif" width="100%"><br><br>
      <img src="./index_files/images/bair_action_free_long_random_00051.gif" width="100%"><br>
    </td>
  </tr>
  <tr>
    <td>
      <span style="font-size:18px;float:right">
        <a href="https://people.eecs.berkeley.edu/~alexlee_gk/projects/savp/tables/bair_action_free_long_random/index.html">[Random samples (500 time-steps)]</a>
      </span>
    </td>
  </tr>
</table>
<br>
<table align=center width=750px>
  <tr>
    <td align=center>
      <span style="font-size:22px">KTH human actions dataset</span>
		</td>
  </tr>
  <tr>
    <td align=center>
      <img src="./index_files/images/kth_00092.gif" width="100%"><br><br>
      <img src="./index_files/images/kth_00105.gif" width="100%"><br><br>
      <img src="./index_files/images/kth_00111.gif" width="100%"><br><br>
      <img src="./index_files/images/kth_00004.gif" width="100%"><br><br>
      <img src="./index_files/images/kth_00195.gif" width="100%"><br><br>
      <img src="./index_files/images/kth_00039.gif" width="100%"><br>
		</td>
  </tr>
  <tr>
    <td>
      <span style="font-size:18px;float:right">
        <a href="https://people.eecs.berkeley.edu/~alexlee_gk/projects/savp/tables/kth_all/index.html">[All test set]</a>
        <a href="https://people.eecs.berkeley.edu/~alexlee_gk/projects/savp/tables/kth_random/index.html">[Random samples]</a>
      </span>
    </td>
  </tr>
</table>
<br>
<table align=center width=750px>
  <tr>
    <td align=center>
      <span style="font-size:22px">BAIR action-conditioned robot pushing dataset</span>
    </td>
  </tr>
  <tr>
    <td align=center>
      <img src="./index_files/images/bair_00056.gif" width="100%"><br><br>
      <img src="./index_files/images/bair_00026.gif" width="100%"><br><br>
      <img src="./index_files/images/bair_00108.gif" width="100%"><br><br>
      <img src="./index_files/images/bair_00033.gif" width="100%"><br>
    </td>
  </tr>
  <tr>
    <td>
      <span style="font-size:18px;float:right">
        <a href="https://people.eecs.berkeley.edu/~alexlee_gk/projects/savp/tables/bair_all/index.html">[All test set]</a>
      </span>
    </td>
  </tr>
</table>
<hr>
<center>
	<h1>Acknowledgments</h1>
</center>
<table align=center width=1000px>
  <tr>
    <td>
      We thank Emily Denton for providing pre-trained models and extensive and timely assistance with reproducing the SVG results, and Mohammad Babaeizadeh for providing data for comparisons with SV2P. This research was supported in part by the Army Research Office through the MAST program, the National Science Foundation through IIS-1651843 and IIS-1614653, and hardware donations from NVIDIA. Alex Lee and Chelsea Finn were also supported by the NSF GRFP. Richard Zhang was partially supported by the Adobe Research Fellowship.
    </td>
  </tr>
</table>
<br>
<br>
</body>
</html>